{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKQQDHASe-bz"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn9B_s0jf4W3"
   },
   "source": [
    "**Project Overview**: I've decided to start by creating a \"smart\" gateway or LLM router that will route users to specific models based on prompt characteristics, token count, and creativity requirements.\n",
    "\n",
    "**Technical Implementation Decisions:** Since I'm building a gateway, I don't want to rely on frameworks or other middleware. I believe we should maintain flexibility and control over the entire process, so I'll be using pure APIs. To ensure output stability and parsing ease, I'll be using Pydantic for structured data handling.\n",
    "For demonstration purposes, I'll use OpenAI models, though in production this could include both open-source and closed-source models of various types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqQFDEdz5S7t"
   },
   "source": [
    "# 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaCdSQNX5YFH"
   },
   "source": [
    "First, we need to import all necessary libraries and datasets, and connect to APIs. To run and use this notebook, you should have your own Hugging Face and OpenAI APIs. The datasets used can be found in the Git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtzvmE3Z7tCT"
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "68f18f1b8bde4d64b226288ebaebb91d",
      "6ff508c9c7b64846a2cd772379a42318",
      "13aa127110da43fd81d46b89a0d03fb6",
      "c01cdb2a2d76463c9cf0bed4eb1f6e26",
      "a38ab276e64f475a9027a1b88dadcc6a",
      "e291db645ddb44949c50c80ae39af550",
      "61e9246c54eb430e99776b2ff7047c3e",
      "9321bce3aebc4e1080666c3c57405cfc",
      "c8b6d58b1c024556ac6e2b32a57dffdf",
      "45dcc3e13d4b4de6b91962d1e7e90ecf",
      "a96af38b09de444aa16d240277d81057",
      "27926b1021ba4087918e4bc6b745f2f0",
      "be1af0cf571140d883e1b6a4c91924f7",
      "40c795340f1e48c8b40513d14bf417ad",
      "63243e3d18494e14977000cf78eb2149",
      "988feec12652443fb8c7db48714cd9c2",
      "33300fa0ad304c4092258a5f3571581c",
      "88610d345e55433facb95d484db9195c",
      "5832184972f14badbecea31c2e78a468",
      "bc5dd8ba4d8e4b55ac4644b921084faa"
     ]
    },
    "id": "43av23hU76Sq",
    "outputId": "a776ac5f-24b2-4b56-fe31-f4c18c371c2f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f18f1b8bde4d64b226288ebaebb91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AI\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "\n",
    "# Pydantic and Formating\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Optional, Literal, Tuple, Dict, Any\n",
    "\n",
    "# Data Manipulations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "# Other\n",
    "import os\n",
    "from google.colab import userdata\n",
    "import base64\n",
    "import time\n",
    "import re\n",
    "\n",
    "client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Upload Datasets Training Dataset\n",
    "file_path = \"/content/Train DataFrame.csv\"\n",
    "model_train_df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Upload Chat Bot Arena Dataset\n",
    "notebook_login()\n",
    "\n",
    "chat_bot_arena_df = pd.read_parquet(\n",
    "    \"hf://datasets/lmsys/chatbot_arena_conversations/data/train-00000-of-00001-cced8514c7ed782a.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot29Df7DtdCL"
   },
   "source": [
    "# 2 Initial Approach\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ1US2aPt0o4"
   },
   "source": [
    "This section will consist of two parts: a user-like interaction where you can interact with the smart router as a user by passing different queries and receiving results, and a second part where I will run tests on the MMLU benchmark to see how our router performs, which model it selects, and what results we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1M_83x-Gtj5i"
   },
   "source": [
    "## 2.1 Smart Router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGjciXhCthkc"
   },
   "source": [
    "**Current Implementation**:I've added analysis functionality to understand why specific models were assigned - this will help us better understand model output and routing decisions. I've started with many components in the current code, but in the next steps I'll clean up and remove unnecessary code to streamline the implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ItJ2ptyJaKBd",
    "outputId": "1d91a336-0edb-4522-ea98-da2989d04d5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: what is capital of france?\n",
      "Enter the temperature or leave blank for default, default is [0.7]: \n",
      "analysis=Analysis(task_type='factual question', complexity_score=1, domain='geography', performance_needs='speed', estimated_tokens=6, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.8, reasoning='The question is a simple factual query with low complexity and knowledge needs, requiring quick response and minimal creativity, making gpt-4.1-nano suitable.')\n",
      "gpt-4.1-nano\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "class Analysis(BaseModel):\n",
    "    \"\"\"Analysis of the user query to determine routing requirements.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(str_strip_whitespace=True)\n",
    "\n",
    "    task_type: str\n",
    "    complexity_score: int = Field(ge=1, le=5)\n",
    "    domain: str\n",
    "    performance_needs: Literal[\"speed\", \"quality\"]\n",
    "    estimated_tokens: int = Field(gt=0)\n",
    "    creativity: Literal[\"low\", \"medium\", \"high\"]\n",
    "\n",
    "\n",
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Decision about which LLM to use for the query.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(str_strip_whitespace=True)\n",
    "\n",
    "    recommended_llm: Literal[\"gpt-4.1-nano\", \"gpt-4.1\", \"o4-mini\"]\n",
    "    confidence: float = Field(ge=0.0, le=1.0)\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "class EvaluationResponse(BaseModel):\n",
    "    \"\"\"Complete evaluation response containing analysis and routing decision.\"\"\"\n",
    "\n",
    "    analysis: Analysis\n",
    "    routing_decision: RoutingDecision\n",
    "\n",
    "\n",
    "class RouteLLM:\n",
    "\n",
    "    def __init__(self, query: str, temperature: float, client: OpenAI) -> None:\n",
    "        self.query = query\n",
    "        self.temperature = temperature\n",
    "        self.client = client\n",
    "\n",
    "    def format_evaluation_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Format evaluation prompt which takes into account temperature and number of tokens\n",
    "        \"\"\"\n",
    "\n",
    "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "        num_tokens = len(enc.encode(self.query))\n",
    "\n",
    "        system_prompt = f\"\"\"\n",
    "        ### Context\n",
    "        Evaluate the user query with specific attention to parameters: temperature (`{self.temperature}`) and number of tokens (`{num_tokens}`). These parameters influence creativity and complexity of tasks, affecting model selection.\n",
    "\n",
    "        ### Role\n",
    "        You are an evaluator responsible for choosing the most suitable model for the user query, ensuring alignment with user-provided constraints on temperature and token count.\n",
    "\n",
    "        ### Task\n",
    "        Analyze the User Query Using These Criteria:\n",
    "        1. Task Type: Determine the user's specific goal or need.\n",
    "        2. Complexity: Score between 1 and 5, reflecting reasoning depth needed; token count {num_tokens} should guide complexity assessment.\n",
    "        3. Domain: Assess whether specialized knowledge is required.\n",
    "        4. Performance Needs: Consider if speed or quality is more important, influenced by the balance between high token count and low count settings.\n",
    "        5. Token Count Estimation: Predict token usage based on the complexity and detail expected from the user's specified token count.\n",
    "        6. Creativity Needs: Ascertain the creativity level needed based on temperature {self.temperature}.\n",
    "\n",
    "        Model Selection:\n",
    "        - Choose the model that best fits the task, respecting specified creativity needs and token constraints.\n",
    "\n",
    "        ### Available Models\n",
    "        1. gpt-4.1-nano\n",
    "           - Strengths: Fast, cost-effective\n",
    "           - Best for: Simple Q&A, basic tasks, low token and creativity needs\n",
    "           - Cost: $0.2/1M tokens\n",
    "\n",
    "        2. gpt-4.1\n",
    "           - Strengths: Complex reasoning, problem-solving\n",
    "           - Best for: Detailed coding, nuanced tasks with medium to high token and creativity needs\n",
    "           - Cost: $2.00/1M tokens\n",
    "\n",
    "        3. o4-mini\n",
    "           - Strengths: Superior reasoning, creativity\n",
    "           - Best for: Complex problem-solving, detailed planning with high creativity and token demands\n",
    "           - Cost: $1.10/1M tokens\n",
    "\n",
    "        ### Examples\n",
    "        #### Example 1\n",
    "        Prompt: 'What is 2+2?'\n",
    "        Output:\n",
    "        ```json\n",
    "        {{\n",
    "            \"analysis\": {{\n",
    "                \"task_type\": \"calculation\",\n",
    "                \"complexity_score\": 1,\n",
    "                \"domain\": \"math\",\n",
    "                \"performance_needs\": \"speed\",\n",
    "                \"estimated_tokens\": 10,\n",
    "                \"creativity\": \"low\"\n",
    "            }},\n",
    "            \"routing_decision\": {{\n",
    "                \"recommended_llm\": \"gpt-4.1-nano\",\n",
    "                \"confidence\": 0.8,\n",
    "                \"reasoning\": \"The task is simple with low token and creativity needs, suitable for gpt-4.1-nano.\"\n",
    "            }}\n",
    "        }}\n",
    "        ```\n",
    "\n",
    "        #### Example 2\n",
    "        Prompt: 'Write Python code for a basic analysis tool using langchain.'\n",
    "        Output:\n",
    "        ```json\n",
    "        {{\n",
    "            \"analysis\": {{\n",
    "                \"task_type\": \"coding\",\n",
    "                \"complexity_score\": 3,\n",
    "                \"domain\": \"coding\",\n",
    "                \"performance_needs\": \"quality\",\n",
    "                \"estimated_tokens\": 1642,\n",
    "                \"creativity\": \"medium\"\n",
    "            }},\n",
    "            \"routing_decision\": {{\n",
    "                \"recommended_llm\": \"gpt-4.1\",\n",
    "                \"confidence\": 0.9,\n",
    "                \"reasoning\": \"Given medium creativity and high token count required, gpt-4.1 fits the needs.\"\n",
    "            }}\n",
    "        }}\n",
    "        ```\n",
    "\n",
    "        #### Example 3\n",
    "        Prompt: 'Implement and classification model for my binary problem.'\n",
    "        Output:\n",
    "        ```json\n",
    "        {{\n",
    "            \"analysis\": {{\n",
    "                \"task_type\": \"data science\",\n",
    "                \"complexity_score\": 5,\n",
    "                \"domain\": \"data science\",\n",
    "                \"performance_needs\": \"quality\",\n",
    "                \"estimated_tokens\": 3600,\n",
    "                \"creativity\": \"medium\"\n",
    "            }},\n",
    "            \"routing_decision\": {{\n",
    "                \"recommended_llm\": \"o4-mini\",\n",
    "                \"confidence\": 0.9,\n",
    "                \"reasoning\": \"Given task difficulty, o4-mini fits the needs.\"\n",
    "            }}\n",
    "        }}\n",
    "        ```\n",
    "\n",
    "        ### Constraints\n",
    "        - Perform detailed, step-by-step reasoning, considering the user-defined temperature and token constraints.\n",
    "       \"\"\"\n",
    "\n",
    "        return system_prompt\n",
    "\n",
    "    def get_evaluation_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the model as response based on the prompt\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.format_evaluation_prompt()},\n",
    "                {\"role\": \"user\", \"content\": self.query},\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            response_format=EvaluationResponse,\n",
    "        )\n",
    "\n",
    "        validated_response = response.choices[0].message.parsed\n",
    "        print(validated_response)\n",
    "\n",
    "        return validated_response.routing_decision.recommended_llm\n",
    "\n",
    "    def get_gpt_response(self, model_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the response from routed model\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": self.query}],\n",
    "            temperature=self.temperature,\n",
    "        )\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def orchestrate_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Orchestrate the response from the model\n",
    "        \"\"\"\n",
    "\n",
    "        recommended_llm = self.get_evaluation_response()\n",
    "        print(recommended_llm)\n",
    "\n",
    "        if recommended_llm == \"gpt-4.1-nano\":\n",
    "            return self.get_gpt_response(\"gpt-4.1-nano\"), \"gpt-4.1-nano\"\n",
    "        elif recommended_llm == \"gpt-4.1\":\n",
    "            return self.get_gpt_response(\"gpt-4.1\"), \"gpt-4.1\"\n",
    "        elif recommended_llm == \"o4-mini\":\n",
    "            return self.get_gpt_response(\"o4-mini\"), \"o4-mini\"\n",
    "\n",
    "\n",
    "query = input(\"Enter your query: \")\n",
    "default_temperature = 0.7\n",
    "\n",
    "temp_input = input(\n",
    "    f\"Enter the temperature or leave blank for default, default is [{default_temperature}]: \"\n",
    ")\n",
    "\n",
    "if temp_input.strip() == \"\":\n",
    "    temperature = default_temperature\n",
    "else:\n",
    "    temperature = float(temp_input)\n",
    "\n",
    "llm = RouteLLM(query, temperature, client)\n",
    "llm.orchestrate_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dTW-92bvEOb"
   },
   "source": [
    "The initial approach is working, but it has many issues that should be fixed and addressed:\n",
    "\n",
    "* The initial prompt is too large and uses too many tokens, so we need to shrink it down\n",
    "* We don't take into account files, because users can pass them too, and this will impact our performance\n",
    "* The analysis part is also not necessary in production; however, it gives a good idea to add maybe a classifier of domains or prompt difficulty, which would help us achieve better results with our own models\n",
    "* Lastly, in this approach I don't check the structure of the prompt and if it contains XML. These patterns can indicate that the user will need a more sophisticated model with more complex prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqGQS8CsVRlN"
   },
   "source": [
    "## 2.2 Smart Router Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ptX3ZPTVVma"
   },
   "source": [
    "Next thing I wanted to test is how this router performs, for this purpose I will user several rows from differnt MMLU data set, the logic behind is that with router we will be able to save some money and maintain accuracy. So we will compare passing all the questions to one gpt 4.1 model and distributing questions among the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94mdkBsyDDW-",
    "outputId": "ffb8882c-077b-43d7-f291-3a22d60497e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain: marketing\n",
      "analysis=Analysis(task_type='multiple-choice question about marketing concepts', complexity_score=2, domain='marketing', performance_needs='speed', estimated_tokens=30, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.9, reasoning='The question tests basic marketing knowledge, requiring a straightforward answer with low complexity and minimal creativity, suitable for the faster, cost-effective gpt-4.1-nano model.')\n",
      "gpt-4.1-nano\n",
      "A\n",
      "analysis=Analysis(task_type='multiple-choice question about organizational terminology in marketing', complexity_score=2, domain='marketing', performance_needs='speed', estimated_tokens=40, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.8, reasoning='The question is straightforward and factual, requiring low complexity and minimal creativity, suitable for the faster, cost-effective gpt-4.1-nano model.')\n",
      "gpt-4.1-nano\n",
      "D\n",
      "analysis=Analysis(task_type='multiple-choice question about marketing theory', complexity_score=2, domain='marketing/psychology', performance_needs='speed', estimated_tokens=50, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.8, reasoning='The question is straightforward and factual, requiring low complexity reasoning. A simple model like gpt-4.1-nano is suitable for quick, accurate responses.')\n",
      "gpt-4.1-nano\n",
      "B\n",
      "analysis=Analysis(task_type='multiple-choice question about marketing influence on society groups', complexity_score=2, domain='marketing/sociology', performance_needs='speed', estimated_tokens=25, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.85, reasoning='The question is straightforward, involving knowledge of social influence and vulnerability among societal groups. The answer choice D (Children) is well-known as the most susceptible to reference group influence due to their developmental stage and reliance on outside influences.')\n",
      "gpt-4.1-nano\n",
      "D\n",
      "analysis=Analysis(task_type='multiple-choice question about marketing media', complexity_score=2, domain='marketing', performance_needs='speed', estimated_tokens=40, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.9, reasoning='The question is straightforward and factual, requiring minimal reasoning, suitable for the fast, low-cost model gpt-4.1-nano.')\n",
      "gpt-4.1-nano\n",
      "A\n",
      "Processing domain: machine_learning\n",
      "analysis=Analysis(task_type='probability calculation with smoothing', complexity_score=3, domain='statistics/machine learning', performance_needs='speed', estimated_tokens=20, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1', confidence=0.85, reasoning='The question involves applying Add-1 smoothing to a frequency distribution to estimate probability, which is a standard statistical method. The task is straightforward, requiring basic calculation and understanding of smoothing, suitable for gpt-4.1 given the moderate complexity and low creativity needs.')\n",
      "gpt-4.1\n",
      "B\n",
      "analysis=Analysis(task_type='multiple-choice question about image data augmentation', complexity_score=2, domain='machine learning/computer vision', performance_needs='speed', estimated_tokens=20, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.9, reasoning='The question is straightforward and factual, involving common knowledge in image data augmentation. The low complexity and low creativity requirements suggest that a fast, cost-effective model like gpt-4.1-nano is suitable.')\n",
      "gpt-4.1-nano\n",
      "A\n",
      "analysis=Analysis(task_type='evaluating scientific claims for acceptance', complexity_score=3, domain='machine learning', performance_needs='quality', estimated_tokens=60, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1', confidence=0.9, reasoning='The task involves understanding the validity of claims regarding error metrics in machine learning, which requires some domain knowledge but is primarily an evaluative decision. The complexity is moderate, and the focus is on precision rather than creativity. gpt-4.1 is suitable for this level of technical assessment.')\n",
      "gpt-4.1\n",
      "C\n",
      "analysis=Analysis(task_type='statistical estimation', complexity_score=3, domain='machine learning/statistics', performance_needs='quality', estimated_tokens=80, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1', confidence=0.9, reasoning=\"The question involves applying Hoeffding's inequality to determine the sample size for a specific confidence and error bound in a machine learning context. Given the moderate complexity and the need for precise statistical reasoning, gpt-4.1 is suitable.\")\n",
      "gpt-4.1\n",
      "D\n",
      "analysis=Analysis(task_type='multiple-choice question about decision-tree learning', complexity_score=2, domain='machine learning', performance_needs='speed', estimated_tokens=65, creativity='low') routing_decision=RoutingDecision(recommended_llm='gpt-4.1-nano', confidence=0.9, reasoning='The question involves understanding the implications of multiway splits in decision trees, specifically the computational and overfitting issues. Given the low creativity requirement and straightforward nature of the question, gpt-4.1-nano is suitable.')\n",
      "gpt-4.1-nano\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "class MMLURouterTest:\n",
    "    def __init__(self, temperature: float = 0.2):\n",
    "        self.temperature = temperature\n",
    "        self.client = client\n",
    "        self.choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        self.domains = [\"marketing\", \"machine_learning\"]\n",
    "        self.results_df = pd.DataFrame()\n",
    "\n",
    "    def get_mmlu_data(self, domain: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the MMLU data set for marketing and machine learning\n",
    "        \"\"\"\n",
    "\n",
    "        mmlu_df = pd.DataFrame(load_dataset(\"cais/mmlu\", f\"{domain}\")[\"dev\"])\n",
    "        samples_df = mmlu_df.head(20)\n",
    "        return samples_df\n",
    "\n",
    "    def get_query(self, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Create a query for the MMLU data set test\n",
    "        \"\"\"\n",
    "\n",
    "        subject = row[\"subject\"]\n",
    "        question = row[\"question\"]\n",
    "        choices = row[\"choices\"]\n",
    "\n",
    "        formatted_choices = \"\"\n",
    "        choice_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        for i, choice in enumerate(choices):\n",
    "            formatted_choices += f\"{choice_letters[i]}. {choice}\\n\"\n",
    "\n",
    "        formatted_query = f\"\"\"The following are questions (with answers) about {subject}.\n",
    "\n",
    "        {question}\n",
    "        {formatted_choices.strip()}\n",
    "        Answer with only the letter (A, B, C, or D):\"\"\"\n",
    "\n",
    "        return formatted_query\n",
    "\n",
    "    def get_simple_gpt41_response(\n",
    "        self, query: str, temperature: float = None\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Pass test query to gpt 4.1 without routing and get response\n",
    "        \"\"\"\n",
    "\n",
    "        if temperature is None:\n",
    "            temperature = self.temperature\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=1,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "\n",
    "        return response.choices[0].message.content.strip(), latency\n",
    "\n",
    "    def calculate_accuracy(\n",
    "        self, predictions: pd.Series, true_answers: pd.Series\n",
    "    ) -> float:\n",
    "        \"\"\"Helper method to calculate accuracy\"\"\"\n",
    "\n",
    "        return (predictions == true_answers).mean() * 100\n",
    "\n",
    "    def get_router_model_stats(self, router_models: pd.Series) -> dict:\n",
    "        \"\"\"Helper method to get router model usage statistics\"\"\"\n",
    "\n",
    "        return router_models.value_counts(normalize=True).mul(100).to_dict()\n",
    "\n",
    "    def get_accuracy_and_stats(self) -> pd.DataFrame:\n",
    "        \"\"\"Return accuracy and model usage stats as DataFrame including latency\"\"\"\n",
    "\n",
    "        if self.results_df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Calculate accuracies\n",
    "        router_correct = (\n",
    "            self.results_df[\"router_answer\"] == self.results_df[\"true_answer\"]\n",
    "        ).sum()\n",
    "        simple_correct = (\n",
    "            self.results_df[\"simple_gpt41_answer\"] == self.results_df[\"true_answer\"]\n",
    "        ).sum()\n",
    "        total = len(self.results_df)\n",
    "\n",
    "        router_accuracy = (router_correct / total) * 100\n",
    "        simple_accuracy = (simple_correct / total) * 100\n",
    "\n",
    "        # Calculate model usage percentages\n",
    "        model_counts = self.results_df[\"router_model\"].value_counts()\n",
    "        model_percentages = (model_counts / total * 100).to_dict()\n",
    "\n",
    "        # Calculate average latencies per model\n",
    "        avg_latencies = {}\n",
    "        for model in self.results_df[\"router_model\"].unique():\n",
    "            model_latencies = self.results_df[self.results_df[\"router_model\"] == model][\n",
    "                \"router_latency\"\n",
    "            ]\n",
    "            avg_latencies[f\"{model}_avg_latency\"] = model_latencies.mean()\n",
    "        simple_avg_latency = self.results_df[\"simple_gpt41_latency\"].mean()\n",
    "\n",
    "        metrics = [\n",
    "            \"router_accuracy\",\n",
    "            \"simple_gpt41_accuracy\",\n",
    "            \"simple_gpt41_avg_latency\",\n",
    "        ]\n",
    "        values = [router_accuracy, simple_accuracy, simple_avg_latency]\n",
    "\n",
    "        metrics.extend(\n",
    "            [f\"{model}_usage_percentage\" for model in model_percentages.keys()]\n",
    "        )\n",
    "        values.extend(list(model_percentages.values()))\n",
    "\n",
    "        metrics.extend(list(avg_latencies.keys()))\n",
    "        values.extend(list(avg_latencies.values()))\n",
    "\n",
    "        stats_data = {\"metric\": metrics, \"value\": values}\n",
    "\n",
    "        return pd.DataFrame(stats_data)\n",
    "\n",
    "    def get_detailed_stats(self) -> dict:\n",
    "        \"\"\"Return detailed statistics as a dictionary including latency stats\"\"\"\n",
    "\n",
    "        if self.results_df.empty:\n",
    "            return {}\n",
    "\n",
    "        router_accuracy = self.calculate_accuracy(\n",
    "            self.results_df[\"router_answer\"], self.results_df[\"true_answer\"]\n",
    "        )\n",
    "        simple_gpt41_accuracy = self.calculate_accuracy(\n",
    "            self.results_df[\"simple_gpt41_answer\"], self.results_df[\"true_answer\"]\n",
    "        )\n",
    "\n",
    "        latency_stats = {}\n",
    "        for model in self.results_df[\"router_model\"].unique():\n",
    "            model_latencies = self.results_df[self.results_df[\"router_model\"] == model][\n",
    "                \"router_latency\"\n",
    "            ]\n",
    "            latency_stats[model] = {\n",
    "                \"avg_latency\": model_latencies.mean(),\n",
    "            }\n",
    "\n",
    "        simple_latencies = self.results_df[\"simple_gpt41_latency\"]\n",
    "        latency_stats[\"simple_gpt41\"] = {\n",
    "            \"avg_latency\": simple_latencies.mean(),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"overall_accuracy\": {\n",
    "                \"router\": router_accuracy,\n",
    "                \"simple_gpt41\": simple_gpt41_accuracy,\n",
    "                \"difference\": router_accuracy - simple_gpt41_accuracy,\n",
    "            },\n",
    "            \"router_model_usage\": self.get_router_model_stats(\n",
    "                self.results_df[\"router_model\"]\n",
    "            ),\n",
    "            \"latency_statistics\": latency_stats,\n",
    "            \"total_questions\": len(self.results_df),\n",
    "            \"correct_answers\": {\n",
    "                \"router\": sum(self.results_df[\"router_correct\"]),\n",
    "                \"simple_gpt41\": sum(self.results_df[\"simple_gpt41_correct\"]),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def pass_questions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Pass questions to the router and get the results\n",
    "        \"\"\"\n",
    "\n",
    "        all_results = []\n",
    "\n",
    "        for domain in self.domains:\n",
    "            print(f\"Processing domain: {domain}\")\n",
    "            df = self.get_mmlu_data(domain)\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                query = self.get_query(row)\n",
    "\n",
    "                start_time = time.time()\n",
    "                router = RouteLLM(\"\", query, self.temperature, self.client)\n",
    "                router_answer, router_model = router.orchestrate_response()\n",
    "                end_time = time.time()\n",
    "                router_latency = end_time - start_time\n",
    "\n",
    "                simple_gpt41_answer, simple_gpt41_latency = (\n",
    "                    self.get_simple_gpt41_response(query, self.temperature)\n",
    "                )\n",
    "\n",
    "                true_answer = self.choices[row[\"answer\"]]\n",
    "                router_correct = router_answer == true_answer\n",
    "                simple_gpt41_correct = simple_gpt41_answer == true_answer\n",
    "\n",
    "                result = {\n",
    "                    \"subject\": row[\"subject\"],\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"choices\": row[\"choices\"],\n",
    "                    \"true_answer\": true_answer,\n",
    "                    \"router_model\": router_model,\n",
    "                    \"router_answer\": router_answer,\n",
    "                    \"router_latency\": router_latency,\n",
    "                    \"router_correct\": router_correct,\n",
    "                    \"simple_gpt41_answer\": simple_gpt41_answer,\n",
    "                    \"simple_gpt41_latency\": simple_gpt41_latency,\n",
    "                    \"simple_gpt41_correct\": simple_gpt41_correct,\n",
    "                }\n",
    "                all_results.append(result)\n",
    "\n",
    "        self.results_df = pd.DataFrame(all_results)\n",
    "        return self.results_df\n",
    "\n",
    "\n",
    "tester = MMLURouterTest()\n",
    "results = tester.pass_questions()\n",
    "\n",
    "stats_df = tester.get_accuracy_and_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "1a136q9PE1Ty",
    "outputId": "4d2e101c-10bb-405d-ae6c-005ca6a890a1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"machine_learning\",\n          \"marketing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"To achieve an 0/1 loss estimate that is less than 1 percent of the true 0/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?\",\n          \"In an organization, the group of people tasked with buying decisions is referred to as the _______________.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"true_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"gpt-4.1\",\n          \"gpt-4.1-nano\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.7723572801192287,\n        \"min\": 1.5972073078155518,\n        \"max\": 10.164355516433716,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.349313497543335,\n          6.238023996353149\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2153905934356142,\n        \"min\": 0.2890610694885254,\n        \"max\": 0.9775447845458984,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.640496015548706,\n          0.29294538497924805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d11894d5-5c13-43cb-950b-d7339b5483f3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>router_model</th>\n",
       "      <th>router_answer</th>\n",
       "      <th>router_latency</th>\n",
       "      <th>router_correct</th>\n",
       "      <th>simple_gpt41_answer</th>\n",
       "      <th>simple_gpt41_latency</th>\n",
       "      <th>simple_gpt41_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>_____________ is a natural outcome when combi...</td>\n",
       "      <td>[Geodemographics, Product differentiation., AN...</td>\n",
       "      <td>A</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>A</td>\n",
       "      <td>3.957749</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>0.977545</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marketing</td>\n",
       "      <td>In an organization, the group of people tasked...</td>\n",
       "      <td>[Outsourcing unit., Procurement centre., Chief...</td>\n",
       "      <td>D</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>D</td>\n",
       "      <td>6.238024</td>\n",
       "      <td>True</td>\n",
       "      <td>D</td>\n",
       "      <td>0.292945</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marketing</td>\n",
       "      <td>Which of the following is an assumption in Ma...</td>\n",
       "      <td>[Needs are dependent on culture and also on so...</td>\n",
       "      <td>B</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>B</td>\n",
       "      <td>1.859863</td>\n",
       "      <td>True</td>\n",
       "      <td>B</td>\n",
       "      <td>0.855934</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marketing</td>\n",
       "      <td>The single group within society that is most v...</td>\n",
       "      <td>[The older consumer who feels somewhat left ou...</td>\n",
       "      <td>D</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>D</td>\n",
       "      <td>1.675521</td>\n",
       "      <td>True</td>\n",
       "      <td>D</td>\n",
       "      <td>0.530231</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marketing</td>\n",
       "      <td>Although the content and quality can be as con...</td>\n",
       "      <td>[Care lines., Direct mail., Inserts., Door to ...</td>\n",
       "      <td>D</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>A</td>\n",
       "      <td>2.445542</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>0.289061</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d11894d5-5c13-43cb-950b-d7339b5483f3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d11894d5-5c13-43cb-950b-d7339b5483f3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d11894d5-5c13-43cb-950b-d7339b5483f3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-35fe3275-2a10-4429-8a07-1aad2d25258d\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35fe3275-2a10-4429-8a07-1aad2d25258d')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-35fe3275-2a10-4429-8a07-1aad2d25258d button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     subject                                           question  \\\n",
       "0  marketing   _____________ is a natural outcome when combi...   \n",
       "1  marketing  In an organization, the group of people tasked...   \n",
       "2  marketing   Which of the following is an assumption in Ma...   \n",
       "3  marketing  The single group within society that is most v...   \n",
       "4  marketing  Although the content and quality can be as con...   \n",
       "\n",
       "                                             choices true_answer  \\\n",
       "0  [Geodemographics, Product differentiation., AN...           A   \n",
       "1  [Outsourcing unit., Procurement centre., Chief...           D   \n",
       "2  [Needs are dependent on culture and also on so...           B   \n",
       "3  [The older consumer who feels somewhat left ou...           D   \n",
       "4  [Care lines., Direct mail., Inserts., Door to ...           D   \n",
       "\n",
       "   router_model router_answer  router_latency  router_correct  \\\n",
       "0  gpt-4.1-nano             A        3.957749            True   \n",
       "1  gpt-4.1-nano             D        6.238024            True   \n",
       "2  gpt-4.1-nano             B        1.859863            True   \n",
       "3  gpt-4.1-nano             D        1.675521            True   \n",
       "4  gpt-4.1-nano             A        2.445542           False   \n",
       "\n",
       "  simple_gpt41_answer  simple_gpt41_latency  simple_gpt41_correct  \n",
       "0                   A              0.977545                  True  \n",
       "1                   D              0.292945                  True  \n",
       "2                   B              0.855934                  True  \n",
       "3                   D              0.530231                  True  \n",
       "4                   C              0.289061                 False  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "qWLmoWqY_WE6",
    "outputId": "fa49c163-69ea-4fa7-ba09-9153fa88ac6e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"stats_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"router_accuracy\",\n          \"simple_gpt41_accuracy\",\n          \"gpt-4.1-nano_avg_latency\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39.55615899131939,\n        \"min\": 0.5866320371627808,\n        \"max\": 90.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          80.0,\n          90.0,\n          3.991180385862078\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "stats_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-e7c08d8b-67c2-4d9e-890d-d8157e570ebc\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>router_accuracy</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_gpt41_accuracy</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_gpt41_avg_latency</td>\n",
       "      <td>0.586632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.1-nano_usage_percentage</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1_usage_percentage</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4.1-nano_avg_latency</td>\n",
       "      <td>3.991180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1_avg_latency</td>\n",
       "      <td>2.056517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e7c08d8b-67c2-4d9e-890d-d8157e570ebc')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e7c08d8b-67c2-4d9e-890d-d8157e570ebc button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e7c08d8b-67c2-4d9e-890d-d8157e570ebc');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-4d0b0f17-59bf-4287-a5ef-37e08b67145e\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d0b0f17-59bf-4287-a5ef-37e08b67145e')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-4d0b0f17-59bf-4287-a5ef-37e08b67145e button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_a349a26c-7a40-4ac3-84b4-a39bc261778c\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('stats_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_a349a26c-7a40-4ac3-84b4-a39bc261778c button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('stats_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                          metric      value\n",
       "0                router_accuracy  80.000000\n",
       "1          simple_gpt41_accuracy  90.000000\n",
       "2       simple_gpt41_avg_latency   0.586632\n",
       "3  gpt-4.1-nano_usage_percentage  70.000000\n",
       "4       gpt-4.1_usage_percentage  30.000000\n",
       "5       gpt-4.1-nano_avg_latency   3.991180\n",
       "6            gpt-4.1_avg_latency   2.056517"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC0SnsLHFe6v"
   },
   "source": [
    "From results:\n",
    "\n",
    "* Router accuracy is lower compared to just sending requests directly to GPT-4.1\n",
    "* On the other hand, our router chose GPT-4.1 Nano 70% of the time, which means we would reduce costs since GPT-4.1 costs $2 per 1M tokens.\n",
    "* Since we introduced an additional layer, we have a substantial increase in latency, so when we build a production-ready gateway, we should consider this and implement different latency reduction strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-rMlvbTIGrco"
   },
   "source": [
    "# 3 Router Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k5RrV4YGu_l"
   },
   "source": [
    "The next step was to address all the issues I found during the first iteration. In this step, I implemented the following changes:\n",
    "\n",
    "* Reduced the size of the prompt and improved it using additional tools\n",
    "* Removed the analysis part\n",
    "* Added the ability to handle files, which would also impact model selection\n",
    "* Added prompt checking to identify more sophisticated prompts\n",
    "* Handeled error with temperature pass to o4-mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aev4nMaCFI4M"
   },
   "source": [
    "## 3.1 Improved Router"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrEMmI4OFNbg"
   },
   "source": [
    "Again, first I developed a version with which users could interact, and then I tested it on the MMLU benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "2hb2nWWHDUQe",
    "outputId": "816a0d60-eb2a-4dab-d099-1e85a2d81080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: how to solve this task in best possible way?\n",
      "Enter the temperature or leave blank for default, default is [0.7]: 0.7\n",
      "Enter file path (image/PDF/other, leave blank for none): /content/AI Engineer Homework.pdf\n",
      "gpt-4.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Here\\'s a step-by-step plan to solve the assignment in the **best possible way**, based on the requirements you provided:\\n\\n---\\n\\n## 1. **Understand the Problem and Goal**\\n\\nYou need to design a proof-of-concept **smart routing feature** for an LLM Gateway. This router will decide which LLM to route a prompt to, based on prompt properties and LLM attributes (cost, speed, feature set, etc.).\\n\\n---\\n\\n## 2. **Break Down the Assignment**\\n\\n### **A. Evaluate Prompt Properties**\\n- Identify the key properties of prompts that could affect routing (e.g., prompt length, type of task, required creativity, expected output length, etc.).\\n- Map these to LLM attributes (e.g., some LLMs are faster, some are cheaper, some are more capable at reasoning or creativity).\\n\\n### **B. Create Dataset & Experiment with Fine-Tuning**\\n- Build a **small dataset** of prompts labeled with the \"best\" LLM for each, based on your mapping/criteria.\\n- Experiment with a routing modelâ€”this could be a simple classifier (e.g., logistic regression, decision tree, or a small neural network).\\n- Fine-tune the model to predict the best LLM for a given prompt.\\n\\n### **C. Build API**\\n- Expose your best routing method (prompt engineering or fine-tuned model) as a simple Python API (e.g., using Flask or FastAPI).\\n\\n### **D. Document**\\n- Prepare a well-structured Jupyter Notebook with:\\n    - Data creation\\n    - Experimentation and results\\n    - Code for model training & evaluation\\n\\n### **E. Suggest Next Steps**\\n- Provide recommendations for scaling, future experiments, and productionizing.\\n\\n---\\n\\n## 3. **Detailed Steps and Tools**\\n\\n### **Step 1: Dataset Creation**\\n- Create a CSV or JSON with prompts and their ideal target LLM.\\n- Use a mix of prompt types (question answering, summarization, creative writing, coding, etc.).\\n- Annotate each with the best LLM (based on your own evaluation or simulated constraints).\\n\\n### **Step 2: Feature Engineering**\\n- Extract features from prompts (e.g., length, keywords, expected output type).\\n- Optionally, include LLM meta-data (cost, latency, etc.) as features.\\n\\n### **Step 3: Model Training**\\n- Use scikit-learn for a simple classifier, or try a small neural net with PyTorch or TensorFlow.\\n- Train/test split, evaluate metrics (accuracy, F1, etc.).\\n\\n### **Step 4: API Development**\\n- Use Flask or FastAPI to create an endpoint:\\n    - Input: prompt + any extra meta-data\\n    - Output: suggested LLM\\n\\n### **Step 5: Documentation**\\n- In your Jupyter Notebook:\\n    - Include EDA, feature extraction, model training, results.\\n    - Save and provide the trained model (pickle or ONNX file).\\n    - Add clear instructions for reproducibility.\\n\\n### **Step 6: Bonus: Deployment**\\n- Deploy your API on a free cloud service (e.g., Hugging Face Spaces, Heroku, or Render.com).\\n- Provide demo instructions.\\n\\n### **Step 7: Write Recommendations**\\n- Summarize what worked, what didnâ€™t, and how to improve.\\n- Suggest future work (bigger datasets, more LLMs, advanced features, real user feedback loop).\\n\\n---\\n\\n## 4. **Template Structure**\\n\\n### **A. Code Repo Structure**\\n```\\n/notebooks/\\n    routing_experiments.ipynb\\n/data/\\n    prompts_dataset.csv\\n/model/\\n    routing_model.pkl\\n/api/\\n    app.py  (Flask or FastAPI)\\nREADME.md\\n```\\n\\n### **B. Example Notebook Outline**\\n1. **Introduction & Problem Statement**\\n2. **Dataset Creation & Description**\\n3. **Feature Engineering**\\n4. **Model Selection & Training**\\n5. **Results & Evaluation**\\n6. **Exporting Model**\\n7. **Instructions for API Testing**\\n8. **Conclusions & Recommendations**\\n\\n---\\n\\n## 5. **Best Practices**\\n\\n- **Keep it simple but clear** (focus on demonstrating the process, not perfect results).\\n- **Document everything** (assumptions, decisions, limitations).\\n- **Reproducibility** (random seeds, requirements.txt, download links for models).\\n- **Deployment** (if time allows, but not mandatory).\\n\\n---\\n\\nIf you want a **starter code template** or a **sample dataset structure**, just ask! Let me know which parts you want help with (code, notebook, API, etc.), and I can provide tailored examples.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Decision about which LLM to use for the query.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(str_strip_whitespace=True)\n",
    "\n",
    "    recommended_llm: Literal[\"gpt-4.1-nano\", \"gpt-4.1\", \"o4-mini\"]\n",
    "\n",
    "\n",
    "class EvaluationResponse(BaseModel):\n",
    "    \"\"\"Complete evaluation response containing analysis and routing decision.\"\"\"\n",
    "\n",
    "    routing_decision: RoutingDecision\n",
    "\n",
    "\n",
    "class RouteLLM:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        query: str,\n",
    "        temperature: float,\n",
    "        client: OpenAI,\n",
    "        file_path: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        self.query = query\n",
    "        self.temperature = temperature\n",
    "        self.client = client\n",
    "        self.file_path = file_path\n",
    "        self.file_added = bool(file_path)\n",
    "        self.file_type = self._get_file_type() if self.file_added else None\n",
    "        self.xml_true = \"<\" in query and \">\" in query\n",
    "        self.uploaded_file_id = None\n",
    "\n",
    "    def _get_file_type(self) -> str:\n",
    "        \"\"\"\n",
    "        Checks the file type and returns the type\n",
    "        \"\"\"\n",
    "        if not self.file_path:\n",
    "            return \"\"\n",
    "        ext = os.path.splitext(self.file_path)[1].lower()\n",
    "        if ext in [\".jpg\", \".jpeg\", \".png\", \".gif\", \".bmp\", \".webp\"]:\n",
    "            return \"image\"\n",
    "        elif ext == \".pdf\":\n",
    "            return \"pdf\"\n",
    "        return \"file\"\n",
    "\n",
    "    def _encode_file(self) -> str:\n",
    "        \"\"\"\n",
    "        Encodes the file and returns the base64 string\n",
    "        \"\"\"\n",
    "        if not self.file_path or not os.path.exists(self.file_path):\n",
    "            return None\n",
    "\n",
    "        with open(self.file_path, \"rb\") as file:\n",
    "            return base64.b64encode(file.read()).decode(\"utf-8\")\n",
    "\n",
    "    def _upload_file(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Upload file using Files API and return file ID\n",
    "        \"\"\"\n",
    "        with open(self.file_path, \"rb\") as file:\n",
    "            file_upload = self.client.files.create(file=file, purpose=\"assistants\")\n",
    "            return file_upload.id\n",
    "\n",
    "    def format_evaluation_prompt(self) -> str:\n",
    "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "        num_tokens = len(enc.encode(self.query))\n",
    "\n",
    "        system_prompt = f\"\"\"\n",
    "        ### Context\n",
    "        Evaluate the user query by emphasizing key parameters: task complexity, creativity level (`{self.temperature}`), required token count (`{num_tokens}`), file details (`{self.file_added}`, `{self.file_type}`)\n",
    "        and complex queries (`{self.xml_true}`). These factors and cost are essential in guiding the model selection process effectively.\n",
    "\n",
    "        ### Task\n",
    "        Assign the Suitable AI Model for the Query:\n",
    "        - **gpt-4.1-nano**: Ideal for tasks requiring minimal reasoning and creativity, perfect for straightforward, concise responses and summaries. Extremely cost-effective at $0.2/1M tokens. Not good for file interaction (pdf, images).\n",
    "        - **gpt-4.1**: Best for moderately complex tasks with a need for balanced creativity and reasoning. Handles coding, basic file interactions, and image processing efficiently. Priced at $2.00/1M tokens.\n",
    "        - **o4-mini**: Suited for highly complex tasks involving deep reasoning and significant creativity. Excels in nuanced, structured analysis and comprehensive file and image processing. Available at $1.10/1M tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        return system_prompt\n",
    "\n",
    "    def get_evaluation_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Evaluate the user query and return the recommended model\n",
    "        \"\"\"\n",
    "        response = self.client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.format_evaluation_prompt()},\n",
    "                {\"role\": \"user\", \"content\": self.query},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            response_format=EvaluationResponse,\n",
    "        )\n",
    "\n",
    "        validated_response = response.choices[0].message.parsed\n",
    "        return validated_response.routing_decision.recommended_llm\n",
    "\n",
    "    def get_gpt_response(self, model_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Gets response from the routed model\n",
    "        \"\"\"\n",
    "\n",
    "        messages = []\n",
    "\n",
    "        if self.file_path and os.path.exists(self.file_path):\n",
    "            if self.file_type == \"image\":\n",
    "                base64_image = self._encode_file()\n",
    "                if base64_image:\n",
    "                    ext = os.path.splitext(self.file_path)[1][1:]\n",
    "                    messages.append(\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": self.query},\n",
    "                                {\n",
    "                                    \"type\": \"image_url\",\n",
    "                                    \"image_url\": {\n",
    "                                        \"url\": f\"data:image/{ext};base64,{base64_image}\"\n",
    "                                    },\n",
    "                                },\n",
    "                            ],\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    messages.append({\"role\": \"user\", \"content\": self.query})\n",
    "            elif self.file_type == \"pdf\":\n",
    "                if not self.uploaded_file_id:\n",
    "                    self.uploaded_file_id = self._upload_file()\n",
    "\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": self.query},\n",
    "                            {\n",
    "                                \"type\": \"file\",\n",
    "                                \"file\": {\"file_id\": self.uploaded_file_id},\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                messages.append({\"role\": \"user\", \"content\": self.query})\n",
    "        else:\n",
    "            messages.append({\"role\": \"user\", \"content\": self.query})\n",
    "\n",
    "        params = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "\n",
    "        if not model_name.startswith(\"o4\"):\n",
    "            params[\"temperature\"] = self.temperature\n",
    "\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def orchestrate_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Based on the evaluation response, get the response from the routed model\n",
    "        \"\"\"\n",
    "\n",
    "        recommended_llm = self.get_evaluation_response()\n",
    "        print(recommended_llm)\n",
    "        if recommended_llm == \"gpt-4.1-nano\":\n",
    "            return self.get_gpt_response(\"gpt-4.1-nano\")\n",
    "        elif recommended_llm == \"gpt-4.1\":\n",
    "            return self.get_gpt_response(\"gpt-4.1\")\n",
    "        elif recommended_llm == \"o4-mini\":\n",
    "            return self.get_gpt_response(\"o4-mini\")\n",
    "\n",
    "\n",
    "query = input(\"Enter your query: \")\n",
    "default_temperature = 0.7\n",
    "\n",
    "temp_input = input(\n",
    "    f\"Enter the temperature or leave blank for default, default is [{default_temperature}]: \"\n",
    ")\n",
    "\n",
    "if temp_input.strip() == \"\":\n",
    "    temperature = default_temperature\n",
    "else:\n",
    "    temperature = float(temp_input)\n",
    "\n",
    "file_path = input(\"Enter file path (image/PDF/other, leave blank for none): \")\n",
    "file_path = file_path.strip() if file_path.strip() else None\n",
    "\n",
    "llm = RouteLLM(query, temperature, client, file_path)\n",
    "llm.orchestrate_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIiLIZUuGOR9"
   },
   "source": [
    "## 3.2 Improve Router Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nay8fZXBG3yb"
   },
   "source": [
    "Since in testing I won't pass any files, I have simplified the code just to get output from models and evaluate the improved prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cI8G8eoATtfO"
   },
   "outputs": [],
   "source": [
    "class RoutingDecision(BaseModel):\n",
    "    \"\"\"Decision about which LLM to use for the query.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(str_strip_whitespace=True)\n",
    "\n",
    "    recommended_llm: Literal[\"gpt-4.1-nano\", \"gpt-4.1\", \"o4-mini\"]\n",
    "\n",
    "\n",
    "class EvaluationResponse(BaseModel):\n",
    "    \"\"\"Complete evaluation response containing analysis and routing decision.\"\"\"\n",
    "\n",
    "    routing_decision: RoutingDecision\n",
    "\n",
    "\n",
    "class RouteLLM:\n",
    "\n",
    "    def __init__(self, query: str, temperature: float, client: OpenAI) -> None:\n",
    "        self.query = query\n",
    "        self.temperature = temperature\n",
    "        self.client = client\n",
    "        self.xml_true = \"<\" in query and \">\" in query\n",
    "\n",
    "    def format_evaluation_prompt(self) -> str:\n",
    "        \"\"\"\n",
    "        Format the evaluation prompt based on passed temperature, number of tokens and if the query is complex\n",
    "        \"\"\"\n",
    "\n",
    "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "        num_tokens = len(enc.encode(self.query))\n",
    "\n",
    "        system_prompt = f\"\"\"\n",
    "        ### Context\n",
    "        Evaluate the user query by emphasizing key parameters: task complexity, creativity level (`{self.temperature}`), required token count (`{num_tokens}`) and complex queries (`{self.xml_true}`). These factors and cost are essential in guiding the model selection process effectively.\n",
    "\n",
    "        ### Task\n",
    "        Assign the Suitable AI Model for the Query:\n",
    "        - **gpt-4.1-nano**: Ideal for tasks requiring minimal reasoning and creativity, perfect for straightforward, concise responses and summaries. Extremely cost-effective at $0.2/1M tokens. Never use with files.\n",
    "        - **gpt-4.1**: Best for moderately complex tasks with a need for balanced creativity and reasoning. Handles coding, basic file interactions, and image processing efficiently. Priced at $2.00/1M tokens.\n",
    "        - **o4-mini**: Suited for highly complex tasks involving deep reasoning and significant creativity. Excels in nuanced, structured analysis and comprehensive file and image processing. Available at $1.10/1M tokens.\n",
    "       \"\"\"\n",
    "\n",
    "        return system_prompt\n",
    "\n",
    "    def get_evaluation_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the routing decision from the model\n",
    "        \"\"\"\n",
    "\n",
    "        response = self.client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.format_evaluation_prompt()},\n",
    "                {\"role\": \"user\", \"content\": self.query},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            response_format=EvaluationResponse,\n",
    "        )\n",
    "\n",
    "        validated_response = response.choices[0].message.parsed\n",
    "        return validated_response.routing_decision.recommended_llm\n",
    "\n",
    "    def get_gpt_response(self, model_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the response from routed model\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": self.query}]\n",
    "\n",
    "        params = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "\n",
    "        if not model_name.startswith(\"o4\"):\n",
    "            params[\"temperature\"] = self.temperature\n",
    "\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def orchestrate_response(self) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Orchestrate the response from the routed model\n",
    "        \"\"\"\n",
    "\n",
    "        recommended_llm = self.get_evaluation_response()\n",
    "        print(recommended_llm)\n",
    "\n",
    "        if recommended_llm == \"gpt-4.1-nano\":\n",
    "            response = self.get_gpt_response(\"gpt-4.1-nano\")\n",
    "        elif recommended_llm == \"gpt-4.1\":\n",
    "            response = self.get_gpt_response(\"gpt-4.1\")\n",
    "        elif recommended_llm == \"o4-mini\":\n",
    "            response = self.get_gpt_response(\"o4-mini\")\n",
    "\n",
    "        return response, recommended_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lvyXVx3pWCDb",
    "outputId": "739d0c70-00e8-4ce2-b5ba-cb26adfb859c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain: marketing\n",
      "gpt-4.1-nano\n",
      "gpt-4.1-nano\n",
      "gpt-4.1-nano\n",
      "gpt-4.1-nano\n",
      "gpt-4.1-nano\n",
      "Processing domain: machine_learning\n",
      "gpt-4.1\n",
      "gpt-4.1-nano\n",
      "gpt-4.1\n",
      "gpt-4.1\n",
      "gpt-4.1\n"
     ]
    }
   ],
   "source": [
    "tester = MMLURouterTest()\n",
    "results = tester.pass_questions()\n",
    "\n",
    "stats_df = tester.get_accuracy_and_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "0ySk-wcCWy6v",
    "outputId": "87db2d62-1173-4418-90c6-493280840e3a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"machine_learning\",\n          \"marketing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"To achieve an 0/1 loss estimate that is less than 1 percent of the true 0/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?\",\n          \"In an organization, the group of people tasked with buying decisions is referred to as the _______________.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"true_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"gpt-4.1\",\n          \"gpt-4.1-nano\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.7355820238318245,\n        \"min\": 0.7287092208862305,\n        \"max\": 9.69335126876831,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.9879603385925293,\n          0.8171894550323486\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18206819693979417,\n        \"min\": 0.29306793212890625,\n        \"max\": 0.9210343360900879,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.38990211486816406,\n          0.29306793212890625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-623b05c1-df28-49c2-97d7-82a76baff87d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>router_model</th>\n",
       "      <th>router_answer</th>\n",
       "      <th>router_latency</th>\n",
       "      <th>router_correct</th>\n",
       "      <th>simple_gpt41_answer</th>\n",
       "      <th>simple_gpt41_latency</th>\n",
       "      <th>simple_gpt41_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>_____________ is a natural outcome when combi...</td>\n",
       "      <td>[Geodemographics, Product differentiation., AN...</td>\n",
       "      <td>A</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>A</td>\n",
       "      <td>1.389133</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>0.433570</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marketing</td>\n",
       "      <td>In an organization, the group of people tasked...</td>\n",
       "      <td>[Outsourcing unit., Procurement centre., Chief...</td>\n",
       "      <td>D</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>D</td>\n",
       "      <td>0.817189</td>\n",
       "      <td>True</td>\n",
       "      <td>D</td>\n",
       "      <td>0.293068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marketing</td>\n",
       "      <td>Which of the following is an assumption in Ma...</td>\n",
       "      <td>[Needs are dependent on culture and also on so...</td>\n",
       "      <td>B</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>B</td>\n",
       "      <td>0.728709</td>\n",
       "      <td>True</td>\n",
       "      <td>B</td>\n",
       "      <td>0.408871</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marketing</td>\n",
       "      <td>The single group within society that is most v...</td>\n",
       "      <td>[The older consumer who feels somewhat left ou...</td>\n",
       "      <td>D</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>D</td>\n",
       "      <td>1.034269</td>\n",
       "      <td>True</td>\n",
       "      <td>D</td>\n",
       "      <td>0.921034</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marketing</td>\n",
       "      <td>Although the content and quality can be as con...</td>\n",
       "      <td>[Care lines., Direct mail., Inserts., Door to ...</td>\n",
       "      <td>D</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>A</td>\n",
       "      <td>0.827034</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>0.545155</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-623b05c1-df28-49c2-97d7-82a76baff87d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-623b05c1-df28-49c2-97d7-82a76baff87d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-623b05c1-df28-49c2-97d7-82a76baff87d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-7707e0b4-6c63-4903-87ed-1a9313d04ae2\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7707e0b4-6c63-4903-87ed-1a9313d04ae2')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-7707e0b4-6c63-4903-87ed-1a9313d04ae2 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     subject                                           question  \\\n",
       "0  marketing   _____________ is a natural outcome when combi...   \n",
       "1  marketing  In an organization, the group of people tasked...   \n",
       "2  marketing   Which of the following is an assumption in Ma...   \n",
       "3  marketing  The single group within society that is most v...   \n",
       "4  marketing  Although the content and quality can be as con...   \n",
       "\n",
       "                                             choices true_answer  \\\n",
       "0  [Geodemographics, Product differentiation., AN...           A   \n",
       "1  [Outsourcing unit., Procurement centre., Chief...           D   \n",
       "2  [Needs are dependent on culture and also on so...           B   \n",
       "3  [The older consumer who feels somewhat left ou...           D   \n",
       "4  [Care lines., Direct mail., Inserts., Door to ...           D   \n",
       "\n",
       "   router_model router_answer  router_latency  router_correct  \\\n",
       "0  gpt-4.1-nano             A        1.389133            True   \n",
       "1  gpt-4.1-nano             D        0.817189            True   \n",
       "2  gpt-4.1-nano             B        0.728709            True   \n",
       "3  gpt-4.1-nano             D        1.034269            True   \n",
       "4  gpt-4.1-nano             A        0.827034           False   \n",
       "\n",
       "  simple_gpt41_answer  simple_gpt41_latency  simple_gpt41_correct  \n",
       "0                   A              0.433570                  True  \n",
       "1                   D              0.293068                  True  \n",
       "2                   B              0.408871                  True  \n",
       "3                   D              0.921034                  True  \n",
       "4                   C              0.545155                 False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "oIUpupDKW18r",
    "outputId": "ada46c87-e57e-4277-e062-7f7bb57a6579"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"stats_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"router_accuracy\",\n          \"simple_gpt41_accuracy\",\n          \"gpt-4.1-nano_avg_latency\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40.453985847749195,\n        \"min\": 0.48852829933166503,\n        \"max\": 90.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          90.0,\n          0.48852829933166503,\n          3.3663299083709717\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "stats_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-6d747050-3121-4e28-97d7-da6263713a93\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>router_accuracy</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_gpt41_accuracy</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_gpt41_avg_latency</td>\n",
       "      <td>0.488528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-4.1-nano_usage_percentage</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1_usage_percentage</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4.1-nano_avg_latency</td>\n",
       "      <td>1.020981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1_avg_latency</td>\n",
       "      <td>3.366330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d747050-3121-4e28-97d7-da6263713a93')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6d747050-3121-4e28-97d7-da6263713a93 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6d747050-3121-4e28-97d7-da6263713a93');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-a9b9132e-ed8a-4b60-801e-f3c3725d3179\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a9b9132e-ed8a-4b60-801e-f3c3725d3179')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-a9b9132e-ed8a-4b60-801e-f3c3725d3179 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_1dee0890-0e58-4387-93d1-ee555e6e0240\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('stats_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_1dee0890-0e58-4387-93d1-ee555e6e0240 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('stats_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                          metric      value\n",
       "0                router_accuracy  90.000000\n",
       "1          simple_gpt41_accuracy  90.000000\n",
       "2       simple_gpt41_avg_latency   0.488528\n",
       "3  gpt-4.1-nano_usage_percentage  60.000000\n",
       "4       gpt-4.1_usage_percentage  40.000000\n",
       "5       gpt-4.1-nano_avg_latency   1.020981\n",
       "6            gpt-4.1_avg_latency   3.366330"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wca4OLE6HdFT"
   },
   "source": [
    "From the results:\n",
    "* We have increased the accuracy quality and are now on par with simply passing everything to GPT-4.1; however, 60% of the time we use GPT Nano\n",
    "* Another important thing to evaluate is why o1-mini wasn't selectedâ€”whether it's an issue with the prompt or the tasks are not tailored for this model based on its description\n",
    "* I still lack performance in terms of latency, but we should still have lower usage costs since GPT-4.1 Nano was used more often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eR8IwTRJGdI",
    "outputId": "995e5a54-93a9-48b2-efe3-f3783f7a77c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' _____________ is a natural outcome when combining demographic and geographic variables.', 'In an organization, the group of people tasked with buying decisions is referred to as the _______________.', \" Which of the following is an assumption in Maslow's hierarchy of needs?\", 'The single group within society that is most vulnerable to reference group influence is:', 'Although the content and quality can be as controlled as direct mail, response rates of this medium are lower because of the lack of a personal address mechanism. This media format is known as:', 'A 6-sided die is rolled 15 times and the results are: side 1 comes up 0 times; side 2: 1 time; side 3: 2 times; side 4: 3 times; side 5: 4 times; side 6: 5 times. Based on these results, what is the probability of side 3 coming up when using Add-1 Smoothing?', 'Which image data augmentation is most common for natural images?', 'You are reviewing papers for the Worldâ€™s Fanciest Machine Learning Conference, and you see submissions with the following claims. Which ones would you consider accepting? ', \"To achieve an 0/1 loss estimate that is less than 1 percent of the true 0/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?\", 'Traditionally, when we have a real-valued input attribute during decision-tree learning we consider a binary split according to whether the attribute is above or below some threshold. Pat suggests that instead we should just have a multiway split with one branch for each of the distinct values of the attribute. From the list below choose the single biggest problem with Patâ€™s suggestion:']\n"
     ]
    }
   ],
   "source": [
    "print(list(results[\"question\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J34HYAQmJTJY"
   },
   "source": [
    "There are questions which could be sent to the reasoning model; however, the description could be not clear enough. Hopefully, the fine-tuning will help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLSRj2TisTOv"
   },
   "source": [
    "# 4 Finetuning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gJDurOXJ0Pt"
   },
   "source": [
    "We got quite good results from the initial smart router where I used LLMs to route user queries. Now I will try to fine-tune a small model to see how it performs. Since I don't have many resources, I will use DistilBERT. But before fine-tuning, first I need to create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NhbTL81xKzfD"
   },
   "source": [
    "## 4.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNTfkpxoK3Zj"
   },
   "source": [
    "In order to create a training set, I will use a chatbot arena dataset which consists of different interactions with different models. I will pass this dataset to GPT as a judge, which will decide which class to assign to each query (where the class represents the model). I will also drop violent queries and different hieroglyphics or unrecognized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p39l3mV5G-_3"
   },
   "outputs": [],
   "source": [
    "class ClassName(BaseModel):\n",
    "    \"\"\"Class name for the user query.\"\"\"\n",
    "\n",
    "    model_config = ConfigDict(str_strip_whitespace=True)\n",
    "\n",
    "    class_name: Literal[\"gpt-4.1-nano\", \"gpt-4.1\", \"o4-mini\"]\n",
    "\n",
    "\n",
    "class EvaluationResponse(BaseModel):\n",
    "    \"\"\"Complete evaluation response containing class name.\"\"\"\n",
    "\n",
    "    class_name: ClassName\n",
    "\n",
    "\n",
    "class DataLabel:\n",
    "\n",
    "    def __init__(self, client: OpenAI):\n",
    "        self.client = client\n",
    "\n",
    "    def split_query(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Extract user and assistant queries from conversation strings.\"\"\"\n",
    "\n",
    "        user_queries = []\n",
    "        chatbot_queries = []\n",
    "\n",
    "        for conv_str in df[\"conversation_a_str\"]:\n",
    "            user_query = \"\"\n",
    "            chatbot_query = \"\"\n",
    "\n",
    "            messages = re.findall(\n",
    "                r\"\\{'content':\\s*'(.*?)',\\s*'role':\\s*'(.*?)'\\}\", conv_str, re.DOTALL\n",
    "            )\n",
    "\n",
    "            for content, role in messages:\n",
    "                if role == \"user\" and not user_query:\n",
    "                    user_query = content\n",
    "                elif role == \"assistant\" and not chatbot_query:\n",
    "                    chatbot_query = content\n",
    "\n",
    "            user_queries.append(user_query)\n",
    "            chatbot_queries.append(chatbot_query)\n",
    "\n",
    "        df.loc[:, \"user_query\"] = user_queries\n",
    "        df.loc[:, \"chatbot_query\"] = chatbot_queries\n",
    "        return df\n",
    "\n",
    "    def convert_to_string(self, df: pd.DataFrame, column_list: list) -> pd.DataFrame:\n",
    "        \"\"\"Convert columns to string type.\"\"\"\n",
    "\n",
    "        for column in column_list:\n",
    "            df.loc[:, column + \"_str\"] = df[column].astype(str)\n",
    "        return df\n",
    "\n",
    "    def clean_text_selective(self, text: str) -> str:\n",
    "        \"\"\"Clean text from special characters and new lines.\"\"\"\n",
    "\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "\n",
    "        text = re.sub(r\"[^\\w\\s\\.,!?;:()\\-\\'\\\"]+\", \"\", str(text))\n",
    "        return \" \".join(text.split()).strip()\n",
    "\n",
    "    def prepare_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare data for labeling.\"\"\"\n",
    "\n",
    "        df = self.convert_to_string(df, [\"conversation_a\"])\n",
    "\n",
    "        df = self.split_query(df)\n",
    "        df = df[[\"user_query\"]]\n",
    "\n",
    "        df = df[df[\"user_query\"] != \"\"]\n",
    "\n",
    "        df[\"user_query\"] = df[\"user_query\"].apply(self.clean_text_selective)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def prepare_prompt(self) -> str:\n",
    "        \"\"\"Prepare prompt for labeling.\"\"\"\n",
    "\n",
    "        system_prompt = \"\"\"\n",
    "            <prompt>\n",
    "\n",
    "            ### Context\n",
    "            Assess the complexity of the userâ€™s question by considering its need for reasoning, technical detail, creativity, domain knowledge, and step-by-step processes. Select the least expensive model that meets the requirements.\n",
    "\n",
    "            ### Task\n",
    "            Determine the appropriate AI model to use:\n",
    "            gpt-4.1-nano: For straightforward tasks, basic questions, and formatting without deep reasoning. Cost: $0.2/1M tokens.\n",
    "            gpt-4.1: For advanced analysis, programming, domain expertise, creative outputs, and complex syntheses. Cost: $2.00/1M tokens.\n",
    "            o4-mini: For multi-step reasoning, strategic planning, analytical tasks, and logic problems. Cost: $1.10/1M tokens.\n",
    "\n",
    "            Output the chosen model's name only.\n",
    "\n",
    "            ### Examples\n",
    "            Query: \"What is the capital of France?\" â†’ gpt-4.1-nano\n",
    "            Query: \"Format this date: 20240315\" â†’ gpt-4.1-nano\n",
    "            Query: \"Is 100 > 50?\" â†’ gpt-4.1-nano\n",
    "            Query: \"List 5 colors\" â†’ gpt-4.1-nano\n",
    "            Query: \"What is better BMW or Audi?\" â†’ gpt-4.1-nano\n",
    "\n",
    "            Query: \"Plan optimal route for 5 deliveries across town\" â†’ o4-mini\n",
    "            Query: \"Solve: If Aâ†’B and Bâ†’C, what about Aâ†’C?\" â†’ o4-mini\n",
    "            Query: \"Analyze pros/cons of remote work policy\" â†’ o4-mini\n",
    "            Query: \"How to cross river with fox, chicken, grain?\" â†’ o4-mini\n",
    "\n",
    "            Query: \"Write binary search tree in Python\" â†’ gpt-4.1\n",
    "            Query: \"Explain quantum computing impact on cryptography\" â†’ gpt-4.1\n",
    "            Query: \"Analyze Renaissance art influence on modern design\" â†’ gpt-4.1\n",
    "            Query: \"Design machine learning pipeline for fraud detection\" â†’ gpt-4.1\n",
    "            </prompt>\n",
    "        \"\"\"\n",
    "\n",
    "        return system_prompt\n",
    "\n",
    "    def get_class(self, df: pd.DataFrame) -> list:\n",
    "        \"\"\"Get class name for the user query.\"\"\"\n",
    "\n",
    "        class_names = []\n",
    "        for idx, row in df.iterrows():\n",
    "            response = self.client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4.1\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.prepare_prompt()},\n",
    "                    {\"role\": \"user\", \"content\": row[\"user_query\"]},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                response_format=EvaluationResponse,\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                response.choices[0].message.parsed\n",
    "                and response.choices[0].message.parsed.class_name\n",
    "            ):\n",
    "                class_name = response.choices[0].message.parsed.class_name.class_name\n",
    "            else:\n",
    "                class_name = None\n",
    "\n",
    "            class_names.append(class_name)\n",
    "\n",
    "        df.loc[:, \"class\"] = class_names\n",
    "        return df\n",
    "\n",
    "    def run_class_label(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Run class label for the user query.\"\"\"\n",
    "\n",
    "        df = self.prepare_data(df)\n",
    "        df = self.get_class(df)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "data_label = DataLabel(client)\n",
    "sample_df = chat_bot_arena_df.head(9000)\n",
    "result_df = data_label.run_class_label(sample_df)\n",
    "\n",
    "print(result_df[[\"user_query\", \"class\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqvyZS42LjsB"
   },
   "source": [
    "As a result, I got around 8k classified rowsâ€”you can find the dataset in the Git repository. I will use this dataset to fine-tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBiq5wfTL0MR"
   },
   "source": [
    "## 4.2 DistilBERT Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-dntjJuRPMg"
   },
   "source": [
    "Main components:\n",
    "1. RouterDataset class:\n",
    "\n",
    "A custom dataset that holds the tokenized text queries and their labels (which model should handle them)\n",
    "\n",
    "2. DataPreparation class:\n",
    "\n",
    "Cleans the data by removing empty queries and mapping model names to numbers (0, 1, 2)\n",
    "Splits data into training and validation sets\n",
    "Converts text queries into tokens that DistilBERT can understand\n",
    "Calculates class weights to handle imbalanced data (if one model type appears much more than others)\n",
    "\n",
    "3. WeightedTrainer class:\n",
    "\n",
    "A custom trainer that uses weighted loss to better handle cases where some classes (models) appear more frequently than others in the training data\n",
    "\n",
    "4. Main training function:\n",
    "\n",
    "Sets up DistilBERT with 3 output classes (one for each model)\n",
    "Trains the model for 5 epochs with specific learning settings\n",
    "Uses early stopping to prevent overfitting\n",
    "Saves the trained model and evaluates its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 936
    },
    "id": "xO1uUXLh62ps",
    "outputId": "945ca344-f112-424d-a9ab-c3dc600f2a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample DataFrame:\n",
      "                                             user_query         class\n",
      "0       What is the difference between OpenCL and CUDA?  gpt-4.1-nano\n",
      "1     Why did my parent not invite me to their wedding?  gpt-4.1-nano\n",
      "2                      Fuji vs. Nikon, which is better?  gpt-4.1-nano\n",
      "3                   How to build an arena for chatbots?       gpt-4.1\n",
      "4                                     When is it today?  gpt-4.1-nano\n",
      "...                                                 ...           ...\n",
      "8174  I want you to act as a linux terminal. I will ...  gpt-4.1-nano\n",
      "8175  What is the funniest line from a book that you...       gpt-4.1\n",
      "8176  When I was young, I thought I knew everything ...  gpt-4.1-nano\n",
      "8177               why is Inter a better font than Stag       o4-mini\n",
      "8178  Create a sales training complete with exercise...       gpt-4.1\n",
      "\n",
      "[8179 rows x 2 columns]\n",
      "\n",
      "Shape: (8179, 2)\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1224' max='1224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1224/1224 02:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.652500</td>\n",
       "      <td>0.671749</td>\n",
       "      <td>0.780233</td>\n",
       "      <td>0.780770</td>\n",
       "      <td>0.780233</td>\n",
       "      <td>0.780422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.413900</td>\n",
       "      <td>0.602251</td>\n",
       "      <td>0.790669</td>\n",
       "      <td>0.799948</td>\n",
       "      <td>0.790669</td>\n",
       "      <td>0.793987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.485600</td>\n",
       "      <td>0.597643</td>\n",
       "      <td>0.790055</td>\n",
       "      <td>0.805427</td>\n",
       "      <td>0.790055</td>\n",
       "      <td>0.794775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "eval_loss: 0.5976\n",
      "eval_accuracy: 0.7901\n",
      "eval_precision: 0.8054\n",
      "eval_recall: 0.7901\n",
      "eval_f1: 0.7948\n",
      "eval_runtime: 1.4037\n",
      "eval_samples_per_second: 1160.4700\n",
      "eval_steps_per_second: 36.3310\n",
      "epoch: 3.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "gpt-4.1-nano     0.8976    0.8056    0.8491       957\n",
      "     gpt-4.1     0.7233    0.7913    0.7558       436\n",
      "     o4-mini     0.5836    0.7246    0.6465       236\n",
      "\n",
      "    accuracy                         0.7901      1629\n",
      "   macro avg     0.7348    0.7738    0.7505      1629\n",
      "weighted avg     0.8054    0.7901    0.7948      1629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class RouterDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for model routing\"\"\"\n",
    "\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.input_ids = tokenized_data[\"input_ids\"]\n",
    "        self.attention_mask = tokenized_data[\"attention_mask\"]\n",
    "        self.labels = tokenized_data[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"attention_mask\": self.attention_mask[idx],\n",
    "            \"labels\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class DataPreparation:\n",
    "    \"\"\"Handles all data preparation steps for router training\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokenizer_name: str = \"distilbert-base-cased\",\n",
    "        max_length: int = 128,\n",
    "    ):\n",
    "        self.df = df.copy()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.class_weights = None\n",
    "        self.label_mapping = {\"gpt-4.1-nano\": 0, \"gpt-4.1\": 1, \"o4-mini\": 2}\n",
    "\n",
    "    def clean_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Clean and prepare data for training\"\"\"\n",
    "\n",
    "        self.df = self.df.dropna(subset=[\"class\", \"user_query\"])\n",
    "\n",
    "        self.df = self.df[self.df[\"user_query\"].str.strip() != \"\"]\n",
    "        self.df = self.df.drop_duplicates(subset=[\"user_query\"], keep=\"first\")\n",
    "\n",
    "        self.df[\"class\"] = self.df[\"class\"].map(self.label_mapping)\n",
    "\n",
    "        self.df = self.df.dropna(subset=[\"class\"])\n",
    "        self.df[\"class\"] = self.df[\"class\"].astype(int)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def split_data(\n",
    "        self, test_size: float = 0.2, random_state: int = 42\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Split data into train and validation sets\"\"\"\n",
    "\n",
    "        self.train_df, self.val_df = train_test_split(\n",
    "            self.df,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=self.df[\"class\"],\n",
    "        )\n",
    "\n",
    "        self.train_df = self.train_df.reset_index(drop=True)\n",
    "        self.val_df = self.val_df.reset_index(drop=True)\n",
    "\n",
    "        return self.train_df, self.val_df\n",
    "\n",
    "    def calculate_class_weights(self) -> dict:\n",
    "        \"\"\"Calculate class weights from training data only\"\"\"\n",
    "\n",
    "        classes = np.unique(self.train_df[\"class\"])\n",
    "        weights = compute_class_weight(\n",
    "            \"balanced\", classes=classes, y=self.train_df[\"class\"]\n",
    "        )\n",
    "        self.class_weights = dict(zip(classes, weights))\n",
    "        return self.class_weights\n",
    "\n",
    "    def tokenize_data(self, df: pd.DataFrame) -> dict:\n",
    "        \"\"\"Tokenize text data\"\"\"\n",
    "\n",
    "        queries = df[\"user_query\"].tolist()\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            queries,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"],\n",
    "            \"attention_mask\": encoded[\"attention_mask\"],\n",
    "            \"labels\": torch.tensor(df[\"class\"].values, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def prepare_data(self) -> tuple[RouterDataset, RouterDataset]:\n",
    "        \"\"\"Complete data preparation pipeline\"\"\"\n",
    "\n",
    "        self.clean_data()\n",
    "        self.split_data()\n",
    "        self.calculate_class_weights()\n",
    "\n",
    "        train_tokenized = self.tokenize_data(self.train_df)\n",
    "        val_tokenized = self.tokenize_data(self.val_df)\n",
    "\n",
    "        self.train_dataset = RouterDataset(train_tokenized)\n",
    "        self.val_dataset = RouterDataset(val_tokenized)\n",
    "\n",
    "        return self.train_dataset, self.val_dataset\n",
    "\n",
    "    def create_dataloaders(\n",
    "        self, batch_size: int = 16, num_workers: int = 0\n",
    "    ) -> tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Create PyTorch dataloaders\"\"\"\n",
    "\n",
    "        if self.train_dataset is None or self.val_dataset is None:\n",
    "            self.prepare_data()\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "        self.val_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "\n",
    "        return self.train_loader, self.val_loader\n",
    "\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with weighted loss for imbalanced classes\"\"\"\n",
    "\n",
    "    def __init__(self, class_weights=None, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        if class_weights is not None:\n",
    "            num_classes = len(class_weights)\n",
    "            weight_list = [class_weights.get(i, 1.0) for i in range(num_classes)]\n",
    "            self.class_weights = torch.tensor(weight_list, dtype=torch.float32)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        inputs: Dict[str, torch.Tensor],\n",
    "        return_outputs: bool = False,\n",
    "        **kwargs: Any,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Compute weighted cross-entropy loss\"\"\"\n",
    "\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights.to(model.device)\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred) -> dict:\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"weighted\"\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def train_router_model(\n",
    "    df: pd.DataFrame, output_dir: str = \"./router_model\"\n",
    ") -> tuple[WeightedTrainer, DataPreparation]:\n",
    "    \"\"\"Main training function\"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    data_prep = DataPreparation(\n",
    "        df=df, tokenizer_name=\"distilbert-base-cased\", max_length=128\n",
    "    )\n",
    "\n",
    "    train_dataset, val_dataset = data_prep.prepare_data()\n",
    "    class_weights = data_prep.class_weights\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-cased\",\n",
    "        num_labels=3,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        dropout=0.3,\n",
    "        attention_dropout=0.1,\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=data_prep.tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=2e-5,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        weight_decay=0.1,\n",
    "        warmup_steps=100,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            EarlyStoppingCallback(\n",
    "                early_stopping_patience=2, early_stopping_threshold=0.01\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model()\n",
    "    data_prep.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"\\nValidation Results:\")\n",
    "    for key, value in eval_results.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "    true_labels = predictions.label_ids\n",
    "\n",
    "    reverse_label_mapping = {v: k for k, v in data_prep.label_mapping.items()}\n",
    "    class_names = [\n",
    "        reverse_label_mapping[i] for i in range(len(data_prep.label_mapping))\n",
    "    ]\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            true_labels, predicted_labels, target_names=class_names, digits=4\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return trainer, data_prep\n",
    "\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "print(model_train_df)\n",
    "print(f\"\\nShape: {model_train_df.shape}\")\n",
    "\n",
    "trainer, data_prep = train_router_model(model_train_df, output_dir=\"./router_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdUmb47N5b8g"
   },
   "source": [
    "From the results:\n",
    "* Overall, we got decent results with a small model and an imbalanced dataset, but there is a clear issue with the o1-mini model, so we would need to bring more samples\n",
    "* In addition, such an approach to fine-tuning is not the best since the data could be different from our target users. Also, as I wrote before, results could be enhanced by first predicting domain and complexity and then moving to the model classification task\n",
    "* Another possibility is to move in the direction of RouterLLM where the class is predicted and the likeliest model probability is calculated, rather predicted exact model. However, their approach considers only two models, so it should be updated\n",
    "* Due to an insufficient amount of data, I get overfitting with a larger number of epochs. For this purpose, new data could be augmented or we could create a bigger dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jwsh9WjsFeZ"
   },
   "source": [
    "## 4.3 DistilBERT As Router Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KsKpAQi36rCc"
   },
   "source": [
    "Lastly we need evaluete our performance, and see if we have any of improvemtn in terms of latency, model selection, accuracy and overl results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlDsokfDBuoB"
   },
   "outputs": [],
   "source": [
    "class RouteLLM:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        query: str,\n",
    "        temperature: float,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        client: OpenAI,\n",
    "    ) -> None:\n",
    "        self.model_path = model_path\n",
    "        self.query = query\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.client = client\n",
    "        self.xml_true = \"<\" in query and \">\" in query\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def predict_single_query(self, query: str) -> tuple[str, float]:\n",
    "        \"\"\"Make prediction for a single query\"\"\"\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            query, padding=True, truncation=True, max_length=128, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            predicted_class = torch.argmax(predictions, dim=-1)\n",
    "\n",
    "        label_names = {0: \"gpt-4.1-nano\", 1: \"gpt-4.1\", 2: \"o4-mini\"}\n",
    "        predicted_label = label_names[predicted_class.item()]\n",
    "        confidence = predictions[0][predicted_class].item()\n",
    "\n",
    "        return predicted_label, confidence\n",
    "\n",
    "    def get_evaluation_response(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the predicted model\n",
    "        \"\"\"\n",
    "\n",
    "        predicted_model, confidence = self.predict_single_query(self.query)\n",
    "        return predicted_model\n",
    "\n",
    "    def get_gpt_response(self, model_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the response from router model\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": self.query}]\n",
    "\n",
    "        params = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "\n",
    "        if not model_name.startswith(\"o4\"):\n",
    "            params[\"temperature\"] = self.temperature\n",
    "\n",
    "        response = self.client.chat.completions.create(**params)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def orchestrate_response(self) -> tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Orchestrate the response from the router model\n",
    "        \"\"\"\n",
    "\n",
    "        recommended_llm = self.get_evaluation_response()\n",
    "        if recommended_llm == \"gpt-4.1-nano\":\n",
    "            response = self.get_gpt_response(\"gpt-4.1-nano\")\n",
    "        elif recommended_llm == \"gpt-4.1\":\n",
    "            response = self.get_gpt_response(\"gpt-4.1\")\n",
    "        elif recommended_llm == \"o4-mini\":\n",
    "            response = self.get_gpt_response(\"o4-mini\")\n",
    "\n",
    "        return response, recommended_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQTWCYmcBulj",
    "outputId": "60782154-1bd3-47b1-aef2-e268e4a4c4ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing domain: marketing\n",
      "Processing domain: machine_learning\n"
     ]
    }
   ],
   "source": [
    "class MMLURouterTest:\n",
    "    def __init__(self, model, tokenizer, temperature: float = 0.2):\n",
    "        self.temperature = temperature\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        self.domains = [\"marketing\", \"machine_learning\"]\n",
    "        self.results_df = pd.DataFrame()\n",
    "\n",
    "    def get_mmlu_data(self, domain: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the MMLU data for marketing and machine learning domains\n",
    "        \"\"\"\n",
    "\n",
    "        mmlu_df = pd.DataFrame(load_dataset(\"cais/mmlu\", f\"{domain}\")[\"dev\"])\n",
    "        samples_df = mmlu_df.head(20)\n",
    "        return samples_df\n",
    "\n",
    "    def get_query(self, row: pd.Series) -> str:\n",
    "        \"\"\"\n",
    "        Create a query for the MMLU data\n",
    "        \"\"\"\n",
    "\n",
    "        subject = row[\"subject\"]\n",
    "        question = row[\"question\"]\n",
    "        choices = row[\"choices\"]\n",
    "\n",
    "        formatted_choices = \"\"\n",
    "        choice_letters = [\"A\", \"B\", \"C\", \"D\"]\n",
    "        for i, choice in enumerate(choices):\n",
    "            formatted_choices += f\"{choice_letters[i]}. {choice}\\n\"\n",
    "\n",
    "        formatted_query = f\"\"\"The following are questions (with answers) about {subject}.\n",
    "\n",
    "        {question}\n",
    "        {formatted_choices.strip()}\n",
    "        Answer with only the letter (A, B, C, or D):\"\"\"\n",
    "\n",
    "        return formatted_query\n",
    "\n",
    "    def get_simple_gpt41_response(\n",
    "        self, query: str, temperature: float = None\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Get the response from the simple GPT-4.1 model\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"gpt-4.1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": query}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=1,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "\n",
    "        return response.choices[0].message.content.strip(), latency\n",
    "\n",
    "    def get_accuracy_and_stats(self):\n",
    "        \"\"\"Return accuracy and model usage stats as DataFrame including latency\"\"\"\n",
    "\n",
    "        if self.results_df.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        router_correct = (\n",
    "            self.results_df[\"router_answer\"] == self.results_df[\"true_answer\"]\n",
    "        ).sum()\n",
    "        simple_correct = (\n",
    "            self.results_df[\"simple_gpt41_answer\"] == self.results_df[\"true_answer\"]\n",
    "        ).sum()\n",
    "        total = len(self.results_df)\n",
    "\n",
    "        router_accuracy = (router_correct / total) * 100\n",
    "        simple_accuracy = (simple_correct / total) * 100\n",
    "\n",
    "        model_counts = self.results_df[\"router_model\"].value_counts()\n",
    "        model_percentages = (model_counts / total * 100).to_dict()\n",
    "\n",
    "        avg_latencies = {}\n",
    "        for model in self.results_df[\"router_model\"].unique():\n",
    "            model_latencies = self.results_df[self.results_df[\"router_model\"] == model][\n",
    "                \"router_latency\"\n",
    "            ]\n",
    "            avg_latencies[f\"{model}_avg_latency\"] = model_latencies.mean()\n",
    "\n",
    "        simple_avg_latency = self.results_df[\"simple_gpt41_latency\"].mean()\n",
    "\n",
    "        metrics = [\n",
    "            \"router_accuracy\",\n",
    "            \"simple_gpt41_accuracy\",\n",
    "            \"simple_gpt41_avg_latency\",\n",
    "        ]\n",
    "        values = [router_accuracy, simple_accuracy, simple_avg_latency]\n",
    "\n",
    "        metrics.extend(\n",
    "            [f\"{model}_usage_percentage\" for model in model_percentages.keys()]\n",
    "        )\n",
    "        values.extend(list(model_percentages.values()))\n",
    "\n",
    "        metrics.extend(list(avg_latencies.keys()))\n",
    "        values.extend(list(avg_latencies.values()))\n",
    "\n",
    "        stats_data = {\"metric\": metrics, \"value\": values}\n",
    "\n",
    "        return pd.DataFrame(stats_data)\n",
    "\n",
    "    def get_detailed_stats(self):\n",
    "        \"\"\"Return detailed statistics as a dictionary including latency stats\"\"\"\n",
    "\n",
    "        if self.results_df.empty:\n",
    "            return {}\n",
    "\n",
    "        router_accuracy = self.calculate_accuracy(\n",
    "            self.results_df[\"router_answer\"], self.results_df[\"true_answer\"]\n",
    "        )\n",
    "        simple_gpt41_accuracy = self.calculate_accuracy(\n",
    "            self.results_df[\"simple_gpt41_answer\"], self.results_df[\"true_answer\"]\n",
    "        )\n",
    "\n",
    "        latency_stats = {}\n",
    "        for model in self.results_df[\"router_model\"].unique():\n",
    "            model_latencies = self.results_df[self.results_df[\"router_model\"] == model][\n",
    "                \"router_latency\"\n",
    "            ]\n",
    "            latency_stats[model] = {\n",
    "                \"avg_latency\": model_latencies.mean(),\n",
    "            }\n",
    "\n",
    "        simple_latencies = self.results_df[\"simple_gpt41_latency\"]\n",
    "        latency_stats[\"simple_gpt41\"] = {\n",
    "            \"avg_latency\": simple_latencies.mean(),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"overall_accuracy\": {\n",
    "                \"router\": router_accuracy,\n",
    "                \"simple_gpt41\": simple_gpt41_accuracy,\n",
    "                \"difference\": router_accuracy - simple_gpt41_accuracy,\n",
    "            },\n",
    "            \"router_model_usage\": self.get_router_model_stats(\n",
    "                self.results_df[\"router_model\"]\n",
    "            ),\n",
    "            \"latency_statistics\": latency_stats,\n",
    "            \"total_questions\": len(self.results_df),\n",
    "            \"correct_answers\": {\n",
    "                \"router\": sum(self.results_df[\"router_correct\"]),\n",
    "                \"simple_gpt41\": sum(self.results_df[\"simple_gpt41_correct\"]),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def pass_questions(self):\n",
    "        all_results = []\n",
    "\n",
    "        for domain in self.domains:\n",
    "            print(f\"Processing domain: {domain}\")\n",
    "            df = self.get_mmlu_data(domain)\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                query = self.get_query(row)\n",
    "\n",
    "                start_time = time.time()\n",
    "                router = RouteLLM(\n",
    "                    \"\", query, self.temperature, self.model, self.tokenizer, self.client\n",
    "                )\n",
    "                router_answer, router_model = router.orchestrate_response()\n",
    "                end_time = time.time()\n",
    "                router_latency = end_time - start_time\n",
    "\n",
    "                simple_gpt41_answer, simple_gpt41_latency = (\n",
    "                    self.get_simple_gpt41_response(query, self.temperature)\n",
    "                )\n",
    "\n",
    "                true_answer = self.choices[row[\"answer\"]]\n",
    "                router_correct = router_answer == true_answer\n",
    "                simple_gpt41_correct = simple_gpt41_answer == true_answer\n",
    "\n",
    "                result = {\n",
    "                    \"subject\": row[\"subject\"],\n",
    "                    \"question\": row[\"question\"],\n",
    "                    \"choices\": row[\"choices\"],\n",
    "                    \"true_answer\": true_answer,\n",
    "                    \"router_model\": router_model,\n",
    "                    \"router_answer\": router_answer,\n",
    "                    \"router_latency\": router_latency,\n",
    "                    \"router_correct\": router_correct,\n",
    "                    \"simple_gpt41_answer\": simple_gpt41_answer,\n",
    "                    \"simple_gpt41_latency\": simple_gpt41_latency,\n",
    "                    \"simple_gpt41_correct\": simple_gpt41_correct,\n",
    "                }\n",
    "                all_results.append(result)\n",
    "\n",
    "        self.results_df = pd.DataFrame(all_results)\n",
    "        return self.results_df\n",
    "\n",
    "    def calculate_accuracy(self, predictions, true_answers):\n",
    "        \"\"\"Helper method to calculate accuracy\"\"\"\n",
    "\n",
    "        return (predictions == true_answers).mean() * 100\n",
    "\n",
    "    def get_router_model_stats(self, router_models):\n",
    "        \"\"\"Helper method to get router model usage statistics\"\"\"\n",
    "\n",
    "        return router_models.value_counts(normalize=True).mul(100).to_dict()\n",
    "\n",
    "\n",
    "tester = MMLURouterTest(trainer.model, data_prep.tokenizer)\n",
    "results = tester.pass_questions()\n",
    "stats_df = tester.get_accuracy_and_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "cx97cfRoFtnu",
    "outputId": "c16c8c97-04d8-49a7-96d1-4e86a80c7f18"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"machine_learning\",\n          \"marketing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"To achieve an 0/1 loss estimate that is less than 1 percent of the true 0/1 loss (with probability 95%), according to Hoeffding's inequality the IID test set must have how many examples?\",\n          \"In an organization, the group of people tasked with buying decisions is referred to as the _______________.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"choices\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"true_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"gpt-4.1\",\n          \"o4-mini\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.038894073509424,\n        \"min\": 0.23117375373840332,\n        \"max\": 11.067426443099976,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          2.9351396560668945,\n          1.9416093826293945\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"router_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"D\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3407800119150048,\n        \"min\": 0.3035752773284912,\n        \"max\": 1.210254192352295,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.49942708015441895,\n          0.36051225662231445\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"simple_gpt41_correct\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a1267c68-01fd-4533-8470-6f756bf706eb\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>true_answer</th>\n",
       "      <th>router_model</th>\n",
       "      <th>router_answer</th>\n",
       "      <th>router_latency</th>\n",
       "      <th>router_correct</th>\n",
       "      <th>simple_gpt41_answer</th>\n",
       "      <th>simple_gpt41_latency</th>\n",
       "      <th>simple_gpt41_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marketing</td>\n",
       "      <td>_____________ is a natural outcome when combi...</td>\n",
       "      <td>[Geodemographics, Product differentiation., AN...</td>\n",
       "      <td>A</td>\n",
       "      <td>gpt-4.1</td>\n",
       "      <td>A</td>\n",
       "      <td>0.522444</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>0.549294</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marketing</td>\n",
       "      <td>In an organization, the group of people tasked...</td>\n",
       "      <td>[Outsourcing unit., Procurement centre., Chief...</td>\n",
       "      <td>D</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>D</td>\n",
       "      <td>1.941609</td>\n",
       "      <td>True</td>\n",
       "      <td>D</td>\n",
       "      <td>0.360512</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>marketing</td>\n",
       "      <td>Which of the following is an assumption in Ma...</td>\n",
       "      <td>[Needs are dependent on culture and also on so...</td>\n",
       "      <td>B</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>B</td>\n",
       "      <td>1.895787</td>\n",
       "      <td>True</td>\n",
       "      <td>B</td>\n",
       "      <td>1.126806</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marketing</td>\n",
       "      <td>The single group within society that is most v...</td>\n",
       "      <td>[The older consumer who feels somewhat left ou...</td>\n",
       "      <td>D</td>\n",
       "      <td>o4-mini</td>\n",
       "      <td>D</td>\n",
       "      <td>2.299963</td>\n",
       "      <td>True</td>\n",
       "      <td>D</td>\n",
       "      <td>1.210254</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>marketing</td>\n",
       "      <td>Although the content and quality can be as con...</td>\n",
       "      <td>[Care lines., Direct mail., Inserts., Door to ...</td>\n",
       "      <td>D</td>\n",
       "      <td>gpt-4.1-nano</td>\n",
       "      <td>A</td>\n",
       "      <td>0.231174</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>0.303575</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1267c68-01fd-4533-8470-6f756bf706eb')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a1267c68-01fd-4533-8470-6f756bf706eb button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a1267c68-01fd-4533-8470-6f756bf706eb');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-e0a7ca4d-fbe2-4e4b-bb69-610cff5634dc\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0a7ca4d-fbe2-4e4b-bb69-610cff5634dc')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-e0a7ca4d-fbe2-4e4b-bb69-610cff5634dc button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "     subject                                           question  \\\n",
       "0  marketing   _____________ is a natural outcome when combi...   \n",
       "1  marketing  In an organization, the group of people tasked...   \n",
       "2  marketing   Which of the following is an assumption in Ma...   \n",
       "3  marketing  The single group within society that is most v...   \n",
       "4  marketing  Although the content and quality can be as con...   \n",
       "\n",
       "                                             choices true_answer  \\\n",
       "0  [Geodemographics, Product differentiation., AN...           A   \n",
       "1  [Outsourcing unit., Procurement centre., Chief...           D   \n",
       "2  [Needs are dependent on culture and also on so...           B   \n",
       "3  [The older consumer who feels somewhat left ou...           D   \n",
       "4  [Care lines., Direct mail., Inserts., Door to ...           D   \n",
       "\n",
       "   router_model router_answer  router_latency  router_correct  \\\n",
       "0       gpt-4.1             A        0.522444            True   \n",
       "1       o4-mini             D        1.941609            True   \n",
       "2       o4-mini             B        1.895787            True   \n",
       "3       o4-mini             D        2.299963            True   \n",
       "4  gpt-4.1-nano             A        0.231174           False   \n",
       "\n",
       "  simple_gpt41_answer  simple_gpt41_latency  simple_gpt41_correct  \n",
       "0                   A              0.549294                  True  \n",
       "1                   D              0.360512                  True  \n",
       "2                   B              1.126806                  True  \n",
       "3                   D              1.210254                  True  \n",
       "4                   C              0.303575                 False  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "id": "qUj2Fa14BueH",
    "outputId": "a493cb4c-9aeb-4c95-bd0e-e67f53a6af7f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"stats_df\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"o4-mini_avg_latency\",\n          \"simple_gpt41_accuracy\",\n          \"gpt-4.1-nano_usage_percentage\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 39.87492900325641,\n        \"min\": 0.23117375373840332,\n        \"max\": 90.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          90.0,\n          80.0,\n          3.3841885924339294\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "stats_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a225839a-a35a-4321-8aab-9b9982859eea\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>router_accuracy</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simple_gpt41_accuracy</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>simple_gpt41_avg_latency</td>\n",
       "      <td>0.688616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>o4-mini_usage_percentage</td>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-4.1_usage_percentage</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpt-4.1-nano_usage_percentage</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpt-4.1_avg_latency</td>\n",
       "      <td>0.522444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>o4-mini_avg_latency</td>\n",
       "      <td>3.384189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt-4.1-nano_avg_latency</td>\n",
       "      <td>0.231174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a225839a-a35a-4321-8aab-9b9982859eea')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a225839a-a35a-4321-8aab-9b9982859eea button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a225839a-a35a-4321-8aab-9b9982859eea');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-a5634d53-2ed2-456a-9c3b-ebcf7136e12b\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a5634d53-2ed2-456a-9c3b-ebcf7136e12b')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-a5634d53-2ed2-456a-9c3b-ebcf7136e12b button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "  <div id=\"id_3e74873e-34d7-4416-9cf1-7988b17178e1\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('stats_df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_3e74873e-34d7-4416-9cf1-7988b17178e1 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('stats_df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                          metric      value\n",
       "0                router_accuracy  90.000000\n",
       "1          simple_gpt41_accuracy  80.000000\n",
       "2       simple_gpt41_avg_latency   0.688616\n",
       "3       o4-mini_usage_percentage  80.000000\n",
       "4       gpt-4.1_usage_percentage  10.000000\n",
       "5  gpt-4.1-nano_usage_percentage  10.000000\n",
       "6            gpt-4.1_avg_latency   0.522444\n",
       "7            o4-mini_avg_latency   3.384189\n",
       "8       gpt-4.1-nano_avg_latency   0.231174"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7C8Ia7G69L1"
   },
   "source": [
    "From Results:\n",
    "* This router achieved same results in terms of accuracy as gpt-4.1-nano and outscored plain gpt-4.1 but since previous runs gpt-4.1 also achieved 90% accuracy I wont consider it\n",
    "* Compared to previous router variants this one majority of the time used o4-mini (80%), which is weird since in training data gpt-4.1-nano was labeled for 60% of queries (4,784/8,142), gpt-4.1 for 27% (2,179), and o4-mini for only 14% (1,179). The router basically flipped the distribution from what GPT-4.1 suggested during labeling. My assumption is that MMLU is not the best dataset to evaluate since each question is academic heavy so router learned to prefer o4-mini over gpt-4.1-nano that was deemed appropriate for most training queries.\n",
    "* Despite the cost effectiveness is still high and accuracy is good the main problem is latency it have increased since o4-mini is used now. Router changed the usage pattern where training labels had mostly gpt-4.1-nano but evaluation shows 80% o4-mini usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hCsjoms_bH2"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5dPnp0C_d_E"
   },
   "source": [
    "Final Conclusion:\n",
    "\n",
    "* The improved LLM-based router using GPT-4.1-nano performs best overall, achieving high accuracy with good latency and cost\n",
    "* The fine-tuned DistilBERT approach shows promise but adds latency due to domain mismatch between training data and MMLU evaluation\n",
    "* Training data must match target use casesâ€”the router preferred o1-mini for academic content despite training on general queries where GPT-4.1-nano was labeled\n",
    "* Next steps include reducing latency through caching, exploring hierarchical classification (domain â†’ complexity â†’ model), and fine-tuning larger 4-8B parameter models\n",
    "* Implementing RouterLLM techniques for multi-model scenarios and hybrid approaches could provide better balance of accuracy, latency, and cost. Future iterations could also explore multi-task learning to train one model on multiple tasks like query classification, domain detection, and complexity assessment, as well as RouterLLM-style approaches that output both classification and probability scores across all three models for better routing decisions"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nexos_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13aa127110da43fd81d46b89a0d03fb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_45dcc3e13d4b4de6b91962d1e7e90ecf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a96af38b09de444aa16d240277d81057",
      "value": ""
     }
    },
    "27926b1021ba4087918e4bc6b745f2f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33300fa0ad304c4092258a5f3571581c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40c795340f1e48c8b40513d14bf417ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45dcc3e13d4b4de6b91962d1e7e90ecf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5832184972f14badbecea31c2e78a468": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61e9246c54eb430e99776b2ff7047c3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "63243e3d18494e14977000cf78eb2149": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "68f18f1b8bde4d64b226288ebaebb91d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_61e9246c54eb430e99776b2ff7047c3e"
     }
    },
    "6ff508c9c7b64846a2cd772379a42318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9321bce3aebc4e1080666c3c57405cfc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c8b6d58b1c024556ac6e2b32a57dffdf",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "88610d345e55433facb95d484db9195c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5832184972f14badbecea31c2e78a468",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bc5dd8ba4d8e4b55ac4644b921084faa",
      "value": "Connecting..."
     }
    },
    "9321bce3aebc4e1080666c3c57405cfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "988feec12652443fb8c7db48714cd9c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a38ab276e64f475a9027a1b88dadcc6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_40c795340f1e48c8b40513d14bf417ad",
      "style": "IPY_MODEL_63243e3d18494e14977000cf78eb2149",
      "tooltip": ""
     }
    },
    "a96af38b09de444aa16d240277d81057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bc5dd8ba4d8e4b55ac4644b921084faa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be1af0cf571140d883e1b6a4c91924f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c01cdb2a2d76463c9cf0bed4eb1f6e26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_27926b1021ba4087918e4bc6b745f2f0",
      "style": "IPY_MODEL_be1af0cf571140d883e1b6a4c91924f7",
      "value": true
     }
    },
    "c8b6d58b1c024556ac6e2b32a57dffdf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e291db645ddb44949c50c80ae39af550": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_988feec12652443fb8c7db48714cd9c2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_33300fa0ad304c4092258a5f3571581c",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
